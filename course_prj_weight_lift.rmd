---
title: 'Practical Machine Learning: Predicting Exercise Quality'
author: "Matthias Schmitt"
date: "8 Januar 2017"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(gbm)
library(knitr)
library(dtplyr)
library(plyr)
library(reshape2)
library(data.table)
library(randomForest)
library(corrplot)
```

## Synopsis

People often quantify how much of an activity they do, but they rarely quantify how well they do it. In this project, we use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to predict the manner in which they did the exercise. The data has been used in the following study with the same goals: <http://groupware.les.inf.puc-rio.br/har>. The study setup is available on that page, too.


```{r echo=TRUE}
set.seed(1234)
train.raw <- read.csv(file="pml-training.csv",head=TRUE,sep=",",na.strings= c("NA",""," "))
test.raw <- read.csv(file="pml-testing.csv",head=TRUE,sep=",",na.strings= c("NA",""," "))
```

## Exploration of the dataset

For univariate data exploration we run a summary of the data and observe the following:

* there are a number of variables with overwhelmingly many missings. Also, some variables are statistical summaries of others (the *kurtosis*-variables etc.).
* many variables have very low standard deviation, which makes them less likely to be good predictors

```{r eval=FALSE}
xtable(summary(train.raw))
# output not shown for reason of length.
```

We use these insights to restrict the number of variables and clean the data.

# Data cleaning & transformation

## Treating missing values

There are a number of columns with (almost) only NA in them (statistical summary variables). We remove variables if they contain $> 50\%$ NAs.

```{r prepare_basedata}
naRows <- colSums(is.na(train.raw))
vars.missings <- names(naRows[naRows >= 0.5*dim(train.raw)[1]])

train.base <- train.raw[,setdiff(names(train.raw),vars.missings)]
```

## Checking data quality for prediction purposes

If a variable $x$ has less than $\sigma(x) < 1$, we'll remove it, for such peaky variables have 
little predictive power. To short before the submission deadline I found **nearZeroVar** from the **caret** package, which does the same thing! Last but not least we remove columns obviously unique to each observation 
in the dataset such as timestamps and ID variables (e.g. participant!).

```{r select_variables, warning=FALSE}
vars.low_sd <- names(train.base[,sapply(train.base, function(x) { sd(x) < 1 })])

## Remove timestamps and ID variables for model training
vars.other <- c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "num_window")
```

## Produce final training set

```{r}
train.final <- train.base[,setdiff(names(train.base),union(vars.low_sd, vars.other))]
```

# Model construction

Given the the large number of variables and data points, we construct a boosting model as a baseline and a random forest as our candidate model for submission. As pointed out in the lecture, most kaggle competitions are won with random forests! First we split our dataset into a train (60%), a train-step validation (40%) and the provided test set:

```{r split_datasets}
inTrain <- createDataPartition(y=train.final$classe,p=.6, list=FALSE)
train.trainingset <- train.final[inTrain,]
train.validationset <- train.final[-inTrain,]
test.outofsample <- test.raw
```

## Identify highly correlated variables 

After univariate analysis already kicked out some variables, another concern for variable selection is correlation. While random forests might be quite robust to correlation in the input variables, it is still better to build a slightly simpler model with fewer variables than to use the last bit of covariation of the training data, which might lead to overfitting. 

```{r compute_correlations}
correlMatrix <- cor(train.trainingset[, -length(train.trainingset)])
corrplot(correlMatrix, order = "FPC", method = "circle", type = "lower", tl.cex = 0.8,  tl.col = rgb(0, 0, 0))
```

We exclude one of a pair of variables $(x_1, x_2)$ if their correlation $\rho(x_1,x_2) > .5$.

```{r identify_high_correlation}
## keep only the lower triangle by 
## filling upper with NA
correlMatrix[upper.tri(correlMatrix, diag=TRUE)] <- NA
m <- melt(correlMatrix)
## sort by descending absolute correlation
m <- m[order(- abs(m$value)), ]
## omit the NA values
correlation <- na.omit(m)
## list of highly correlated variables, take first variable of each pair to eliminate
vars.highcorr <- correlation[correlation$value > .5,"Var1"]

vars.final <- setdiff(names(train.trainingset),vars.highcorr)
```

```{r finalize_trainingdata}
# final variable selection for the training set
train.trainingset.complete <- train.trainingset
train.trainingset.final <- train.trainingset[,vars.final]
```

We can do a sanity check of this reduction in the number of variables by using *rfcv*, which gives us achievable accuracy on a random forest given a subset of the variables in the dataset:

```{r var_selection}
cvresult <- rfcv(train.trainingset.complete[,!names(train.trainingset.complete)=="classe"],   
                 train.trainingset.complete$classe)
plot(names(cvresult$error.cv),  cvresult$error.cv, xlab="# variables used in model", ylab="error")
```

Not much is happening after $> 20$ of the best-predicting variables are employed in a model.

## Fit a model

We establish a baseline accuracy using gradient boosting.

```{r baseline_gbm, echo=TRUE, message=FALSE, results="hide"}
model.gbm.lowcorr <- train(classe ~ ., data = train.trainingset, method="gbm")
```
```{r}
confusionMatrix(train.validationset$classe, predict(model.gbm.lowcorr, train.validationset))
```

Classes B/C seem especially hard to match (sometimes as low as only 88% sensitivity, depending on the run). Let''s see if random forest can do better.

```{r train}
model.lowcorr <- randomForest(classe ~ ., data = train.trainingset.final, keep.inbag=TRUE, keep.forest = TRUE)
```

Again we validate the model using the remaining data of the training set:

```{r validate}
predict.crossval.lowcorr <- predict(model.lowcorr, train.validationset)
confusionMatrix(train.validationset$classe, predict.crossval.lowcorr)  
```

Indeed, now class C reaches satisfactory sensitivity as well.

# Explore the model 

We are curious to see what variables actually define the model. The importance plot shows that *roll belt* is by far the most important variable, in step with the output of *rfcv* above. We have an expected test set error of 1% for this part of the training set (OOB); this value should get better once we use the entire training set.

```{r explore_model}
print(model.lowcorr)
plot(model.lowcorr)
varImpPlot(model.lowcorr,type=2)
```

What's interesting about the importance plot is that variable importance falls almost exponentially - again a sign that our cut-off at 24 variables should not have too much of an impact while simplifying the model. The output of *partialPlot* - the ''*partial derivative*'' of the model to some variable - reveals no interesting or interpretable insights, and is therefore skipped here. 

# Model validation - run on testcases

We re-train the model on the entire training set before we apply the model to the final out-of-sample test set.

```{r retrain}
model.final <- randomForest(classe ~ ., data = train.final[,vars.final], keep.inbag=TRUE, keep.forest = TRUE)
```

```{r run_testcases}
predict.test.result <- predict(model.final, test.outofsample)
print(predict.test.result)
```




